{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# U.S. i94 Immigrant Travel DataWarehouse Build\n",
    "### A Data Engineering Project\n",
    "\n",
    "#### Project Summary\n",
    "This project recieves and prepares I94 Immigrant data from the United States for analysis. This data represents every immigrant departure at every port by date. It also has other pertinent information including the address the immigrant stayed in the US as well as whether the records was matched to an irrival i94 or not. The project intends to include dimensions of state-of-residence ddemographics and average monthly temps at departing port.\n",
    "\n",
    "The model is used to answer analytical questions such as:\n",
    "- What airports handle the most or least number of immigrants in a given month?\n",
    "- Any correlation of immigrant travel with city temperatures at destination? Tourist?\n",
    "- What airports may need more or less immigration officers assigned.\n",
    "- What states are gaining or losing immigrants?\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All imports and installs here\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from decimal import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Model a star schema for the I94 immigrant travel data. The fact table is the aggregation of I94 travel data by month, airport and address. The model has two dimensions\n",
    "1. state_demographics dimension has population data aggregated by state corresponding with the addresses of Immigrant State of residence.\n",
    "2. I94port city details. This dimension details the city the airport is located in, along with weather data for that city aggregated by month where available.\n",
    "\n",
    "The main reason to choose the star schema over other schemas like snowflake is beause of it's faster queries because the fact tables are not normalized. Our city and airport and temperature dimension is denormalized. This is deliberate to increase efficiency of analytical queries with less joins.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The I94 Immigrant travel data is a collection of government immigration data from 2016 departures. It includes the port of departure, date , arrival where available, gender, year of birth and airline details.\n",
    "The city data includes US city and state information including geographic coordinates.\n",
    "US cities demographis information contains population counts for male, females, vets, foreign-born individuals by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Exploring and Assessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Common utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "def getSparkSession():\n",
    "    sparkSession = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "    return sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read SAS7BAT data to Spark DF\n",
    "def readSAS7BDATtoSparkDF(sparksession,fname):\n",
    "    df_spark = sparksession.read.format('com.github.saurfang.sas.spark').load(fname)\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read CSV to Spark DF\n",
    "def readCSVtoSparkDF(sparksession,fname,delimeter_=';',header_=True,inferSchema_=True):\n",
    "    sparkDF = sparksession.read.option(\"delimiter\",delimeter_).option(\"header\",True).option(\"inferSchema\",True).csv(fname)\n",
    "    return sparkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean I94 Travel Data\n",
    "def cleanI94TravelData(I94TravelDataDF):\n",
    "    I94TravelDataDF = I94TravelDataDF[\"i94yr\",\"i94mon\",\"i94port\",\"i94addr\"]\n",
    "    I94TravelDataDF = I94TravelDataDF.groupby(\"i94yr\",\"i94mon\",\"i94port\",\"i94addr\").count()\n",
    "    return I94TravelDataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean city demographics data\n",
    "def cleanCityDemographicsData(cityDemographicsDataDF):\n",
    "    cityDemographicsDataDF = cityDemographicsDataDF.drop('Race','Count','Average Household Size','Median Age','City','State')\n",
    "    cityDemographicsDataDF = cityDemographicsDataDF.drop_duplicates()\n",
    "    cleanedCityDemographicsDataDF = cityDemographicsDataDF \\\n",
    "        .groupby(\"State Code\") \\\n",
    "        .sum(\"Male Population\",\"Female Population\",\"Total Population\",\"Number of Veterans\",\"Foreign-born\") \\\n",
    "        .withColumnRenamed(\"sum(Male Population)\", \"male_population\") \\\n",
    "        .withColumnRenamed(\"sum(Female Population)\", \"female_population\") \\\n",
    "        .withColumnRenamed(\"sum(Total Population)\", \"total_population\") \\\n",
    "        .withColumnRenamed(\"sum(Number of Veterans)\", \"num_of_vets\") \\\n",
    "        .withColumnRenamed(\"sum(Foreign-born)\", \"foreign_born\") \\\n",
    "        .withColumnRenamed(\"State Code\",\"state_code\")\n",
    "    return cleanedCityDemographicsDataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean airport codes data\n",
    "def cleanAirportCodes(airportCodesDF):\n",
    "    airportCodesDF = airportCodesDF.dropna(how='any',subset=['iata_code']).filter(airportCodesDF.iso_country == 'US')\n",
    "    airportCodesDF = airportCodesDF \\\n",
    "        .withColumn('Longitude', split(col(\"coordinates\"),\",\").getItem(0).cast(DoubleType())) \\\n",
    "        .withColumn('Latitude',  split(col(\"coordinates\"),\",\").getItem(1).cast(DoubleType()))\n",
    "    airportCodesDF = airportCodesDF.drop(\"continent\",\"iso_country\",\"coordinates\",\"ident\")\n",
    "    return airportCodesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean global temperature by city data step 1 filter\n",
    "def filterGlobalTemperaturesByCityDF(globalLandTemperaturesByCityDF):\n",
    "    globalLandTemperaturesByCityDF = globalLandTemperaturesByCityDF.dropna(how='any')\n",
    "    globalLandTemperaturesByCityDF = globalLandTemperaturesByCityDF.filter(globalLandTemperaturesByCityDF.Country == \"United States\")\n",
    "    filteredGlobalLandTemperaturesByCityDF = globalLandTemperaturesByCityDF.withColumn('month',month(globalLandTemperaturesByCityDF.dt))\n",
    "    return filteredGlobalLandTemperaturesByCityDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean global temperature by city data step 2 column renames\n",
    "def replaceGlobalLandTemperaturesByCity(filteredGlobalLandTemperaturesByCityDF):\n",
    "    filteredGlobalLandTemperaturesByCityDF = filteredGlobalLandTemperaturesByCityDF \\\n",
    "    .withColumn('absLongitude', (split(col(\"Longitude\"),\"W\").getItem(0).cast(DoubleType())*-1 )) \\\n",
    "    .withColumn('absLatitude',split(col(\"Latitude\"),\"N\").getItem(0).cast(DoubleType()))\n",
    "    replacedGlobalLandTemperaturesByCityDF = filteredGlobalLandTemperaturesByCityDF.drop(\"dt\",\"Longitude\",\"Latitude\")\n",
    "    return replacedGlobalLandTemperaturesByCityDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean global temperature by city data step 3 groupby\n",
    "def groupByGlobalLandTemperaturesByCity(replacedGlobalLandTemperaturesByCityDF):\n",
    "    groupbyGlobalLandTemperaturesByCityDF = replacedGlobalLandTemperaturesByCityDF \\\n",
    "    .groupby(\"month\",\"Country\",\"City\",\"absLongitude\",\"absLatitude\").mean(\"AverageTemperature\",\"AverageTemperatureUncertainty\") \\\n",
    "    .withColumnRenamed(\"avg(AverageTemperature)\",\"avg_temp\") \\\n",
    "    .withColumnRenamed(\"avg(AverageTemperatureUncertainty)\",\"avg_temp_uncertainty\")\n",
    "    return groupbyGlobalLandTemperaturesByCityDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def joinAndCleanupAirportAndTempData(airportCodesDF,groupByGlobalLandTemperaturesByCityDF):\n",
    "    # Left join\n",
    "    i95port_cities_mon_avg_tempsDF = airportCodesDF.join(groupByGlobalLandTemperaturesByCityDF,airportCodesDF.municipality == groupByGlobalLandTemperaturesByCityDF.City,how='left')\n",
    "    # Eliminate any records where City name exists in two or more states.\n",
    "    i95port_cities_mon_avg_tempsDF = i95port_cities_mon_avg_tempsDF \\\n",
    "        .filter( i95port_cities_mon_avg_tempsDF.Longitude.isNull() | ((abs(i95port_cities_mon_avg_tempsDF.absLongitude - i95port_cities_mon_avg_tempsDF.Longitude) < 2.0) \\\n",
    "        & (abs(i95port_cities_mon_avg_tempsDF.absLatitude - i95port_cities_mon_avg_tempsDF.Latitude) < 2.0)))\n",
    "    # Drop unneded columns\n",
    "    i95port_cities_mon_avg_tempsDF = i95port_cities_mon_avg_tempsDF \\\n",
    "        .drop(\"type\",\"elevation_ft\",\"gps_code\",\"local_code\",\"Longitude\",\"Latitude\",\"Country\",\"City\",\"absLongitude\",\"absLatitude\") \\\n",
    "        .withColumnRenamed(\"iso-region\",\"state\").withColumnRenamed(\"municipality\",\"city\").withColumnRenamed(\"iata_code\",\"i94prtl\")\n",
    "    return i95port_cities_mon_avg_tempsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Quality Check confirm existence of a table\n",
    "def checkTableExists(table_name):\n",
    "    list_of_tables = sparkSession.sql(\"show tables\")\n",
    "    if list_of_tables.count() > 0:\n",
    "        if len(list_of_tables.filter(list_of_tables.tableName == 'state_demographics').collect()) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Quality Check confirm existence of a table\n",
    "def checkTableRowCountMatches(table_name,expected_count):\n",
    "    query = f\"SELECT COUNT(*) FROM {table_name}\"\n",
    "    actual_count_df = sparkSession.sql(query)\n",
    "    if actual_count_df.count() > 0:\n",
    "        actual_count_df = actual_count_df.withColumnRenamed(\"count(1)\",\"actualCount\")\n",
    "        if len(actual_count_df.filter(actual_count_df.actualCount == expected_count).collect()) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def checkDistinctCountByColumnMatches(tablename,columnname,expected_count):\n",
    "    query = f\"SELECT DISTINCT {columnname} FROM {tablename}\"\n",
    "    spark_df = spark.sql(query)\n",
    "    if spark_df.count() == expected_count:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Explore the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Explore I94 Travel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get spark session\n",
    "sparkSession = getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read a random month for exploration\n",
    "I94TravelDataDF = readSAS7BDATtoSparkDF(sparkSession,'../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|   2347.0|2016.0|  11.0| 577.0| 577.0|    ATL|20759.0|    1.0|     FL|20852.0|  36.0|    1.0|  1.0|20161101|     GTM| null|      T|      I|   null|      M| 1980.0|04302017|     M|  null|     DL|1.2928767685E10|  906|      B1|\n",
      "|2462421.0|2016.0|  11.0| 135.0| 135.0|    ATL|20772.0|    1.0|     FL|20852.0|  33.0|    2.0|  1.0|20161114|    null| null|      O|      O|   null|      M| 1983.0|02112017|     M|  null|     DL|1.3218916485E10|   31|      WT|\n",
      "|4191149.0|2016.0|  11.0| 528.0| 528.0|    ATL|20780.0|    1.0|     FL|20852.0|  25.0|    2.0|  1.0|20161122|     TGG| null|      G|      O|   null|      M| 1991.0|05212017|     F| 18435|     DL|1.4161122385E10|  552|      B2|\n",
      "|5411302.0|2016.0|  11.0| 260.0| 260.0|    ATL|20786.0|    1.0|     FL|20852.0|   3.0|    2.0|  1.0|20161128|     MNL| null|      O|      O|   null|      M| 2013.0|05272017|     M|  null|     DL|1.4986003185E10|  296|      B2|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore records and compare with provided I94_SAS_Labels_Descriptions.SAS\n",
    "I94TravelDataDF.filter(I94TravelDataDF.i94port == \"ATL\").filter(I94TravelDataDF.i94addr == \"FL\").filter(I94TravelDataDF.airline == \"DL\").filter(I94TravelDataDF.depdate == 20852.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Explore City Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get spark session\n",
    "sparkSession = getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read city demographics data\n",
    "cityDemographicsDataDF = readCSVtoSparkDF(sparkSession,\"us-cities-demographics.csv\",';',True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|  City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n",
      "+------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|Toledo| Ohio|      36.1|         135455|           144323|          279778|             15286|        9257|                  2.29|        OH|American Indian a...|  3942|\n",
      "|Toledo| Ohio|      36.1|         135455|           144323|          279778|             15286|        9257|                  2.29|        OH|  Hispanic or Latino| 23614|\n",
      "|Toledo| Ohio|      36.1|         135455|           144323|          279778|             15286|        9257|                  2.29|        OH|               Asian|  6850|\n",
      "|Toledo| Ohio|      36.1|         135455|           144323|          279778|             15286|        9257|                  2.29|        OH|               White|188325|\n",
      "|Toledo| Ohio|      36.1|         135455|           144323|          279778|             15286|        9257|                  2.29|        OH|Black or African-...| 85552|\n",
      "+------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore a random city (Toledo)\n",
    "cityDemographicsDataDF.filter(cityDemographicsDataDF.City == \"Toledo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Explore Airport Codes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get spark session\n",
    "sparkSession = getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read airport codes data\n",
    "airportCodesDF = readCSVtoSparkDF(sparkSession,\"airport-codes_csv.csv\",',',True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "| KAGS|large_airport|Augusta Regional ...|         144|       NA|         US|     US-GA|      Augusta|    KAGS|      AGS|       AGS|-81.9645004272461...|\n",
      "| KATL|large_airport|Hartsfield Jackso...|        1026|       NA|         US|     US-GA|      Atlanta|    KATL|      ATL|       ATL| -84.428101, 33.6367|\n",
      "| KMGE|large_airport|Dobbins Air Reser...|        1068|       NA|         US|     US-GA|     Marietta|    KMGE|      MGE|       MGE|-84.51629639, 33....|\n",
      "| KSAV|large_airport|Savannah Hilton H...|          50|       NA|         US|     US-GA|     Savannah|    KSAV|      SAV|       SAV|-81.20210266, 32....|\n",
      "| KWRB|large_airport|Robins Air Force ...|         294|       NA|         US|     US-GA|Warner Robins|    KWRB|      WRB|       WRB|-83.5919036865, 3...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample a regions airports\n",
    "airportCodesDF.filter(airportCodesDF.iso_region == \"US-GA\").filter(airportCodesDF.type == \"large_airport\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Explore City Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get spark session\n",
    "sparkSession = getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read airport codes data\n",
    "globalLandTemperaturesByCityDF = readCSVtoSparkDF(sparkSession,'../../data2/GlobalLandTemperaturesByCity.csv',',',True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|                 dt| AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|              6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01 00:00:00| 5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01 00:00:00|             10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01 00:00:00| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01 00:00:00|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-09-01 00:00:00| 12.780999999999999|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-10-01 00:00:00|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-11-01 00:00:00|  4.638999999999999|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-12-01 00:00:00|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-01-01 00:00:00|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-02-01 00:00:00|             -2.732|                        1.358|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-03-01 00:00:00|              0.129|                        1.088|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-04-01 00:00:00|              4.042|                        1.138|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-05-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-06-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sample data\n",
    "globalLandTemperaturesByCityDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Notes:\n",
    "\n",
    "1.  The city and state of the airports is available. But the weather data only states the city. So the state information for the weather is missing. Joining these two will result in ambigouse data since some City names (for example Toledo) appear in more than one state.\n",
    "The only piece of information available to ensure correct alignment of Airport city with it's weather is to also use coordinates. This piece of information is presented differently in the airport codes vs the weather data. Needs formatting to be able to use for joins.\n",
    "\n",
    "2.  There is lots of missing data. For the case of weather any record missing temperature shall be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Steps to clean the I94 Travel Data\n",
    "Main step here is to identify the needed columns. Many of the missing values are on columns not needed by our data model. Then appropriately groupby the required year, month, airport and address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Clean,De-dup,aggregate I94TravelData\n",
    "cleanedI94TravelDataDF = cleanI94TravelData(I94TravelDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------+-----+\n",
      "| i94yr|i94mon|i94port|i94addr|count|\n",
      "+------+------+-------+-------+-----+\n",
      "|2016.0|  11.0|    DET|     RI|   33|\n",
      "+------+------+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate the needed columns are present\n",
    "cleanedI94TravelDataDF.filter(I94TravelDataDF.i94port == \"DET\").filter(I94TravelDataDF.i94addr == \"RI\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Clean City Demographics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "City demographics data is denormalized and has lots of duplicates. The steps to clean are as follows\n",
    "1. Identify the needed fields for out data model (state code, male, female,total, number of vets and foreign born populations)\n",
    "2. Drop unneeded fields and the drop duplicates.\n",
    "3. Finally group by the appropriate field (state code) and sum the populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean city demographics data\n",
    "cleanedCityDemographicsDataDF = cleanCityDemographicsData(cityDemographicsDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+----------------+-----------+------------+\n",
      "|state_code|male_population|female_population|total_population|num_of_vets|foreign_born|\n",
      "+----------+---------------+-----------------+----------------+-----------+------------+\n",
      "|        AZ|        2227455|          2272087|         4499542|     264505|      682313|\n",
      "+----------+---------------+-----------------+----------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample the data\n",
    "cleanedCityDemographicsDataDF.filter(cleanedCityDemographicsDataDF.state_code == \"AZ\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Clean Airport Codes Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As noted during exploration, there are non-US airports as well as non-airports like Hellipads on this data. Cleanup by\n",
    "1. Restricting to US only as we are interested in I94 departures from US.\n",
    "2. Restrict by iata_code as that is the available information for departure port. This is key information, and any travel info without this is not useful for out data model. It is noise that needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean airport codes data\n",
    "cleanedAirportCodesDF = cleanAirportCodes(airportCodesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------+----------+------------+--------+---------+----------+------------+-----------+\n",
      "|         type|                name|elevation_ft|iso_region|municipality|gps_code|iata_code|local_code|   Longitude|   Latitude|\n",
      "+-------------+--------------------+------------+----------+------------+--------+---------+----------+------------+-----------+\n",
      "|large_airport|William P Hobby A...|          46|     US-TX|     Houston|    KHOU|      HOU|       HOU|-95.27890015|29.64539909|\n",
      "+-------------+--------------------+------------+----------+------------+--------+---------+----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample airport\n",
    "cleanedAirportCodesDF.filter(cleanedAirportCodesDF.iata_code == \"HOU\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean City Temperature Data\n",
    "We are only interested in the United States. Clean up steps involve\n",
    "1. Filter and keep only United States data\n",
    "2. Drop any records with null tempreatures are they are meaningless in this context.\n",
    "3. Extract the month and create a column for month as we need to group by month for our data model.\n",
    "4. To handle duplicates, we decide to take the average of the temps and variability index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter, replace columns and finally groupby\n",
    "filteredGlobalLandTemperaturesByCityDF = filterGlobalTemperaturesByCityDF(globalLandTemperaturesByCityDF)\n",
    "replacedGlobalLandTemperaturesByCityDF = replaceGlobalLandTemperaturesByCity(filteredGlobalLandTemperaturesByCityDF)\n",
    "groupByGlobalLandTemperaturesByCityDF = groupByGlobalLandTemperaturesByCity(replacedGlobalLandTemperaturesByCityDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------+------------+-----------+--------------------+--------------------+\n",
      "|month|      Country|        City|absLongitude|absLatitude|            avg_temp|avg_temp_uncertainty|\n",
      "+-----+-------------+------------+------------+-----------+--------------------+--------------------+\n",
      "|    1|United States|  Alexandria|      -76.99|      39.38|-0.35399619771863117|  1.7332661596958165|\n",
      "|    7|United States| Chula Vista|     -117.77|      32.95|  19.191678787878775|  0.8804181818181819|\n",
      "|    5|United States|    Columbus|      -83.24|      39.38|  16.823312499999993|  1.2040703124999985|\n",
      "|   10|United States|    Columbus|      -83.24|      39.38|  11.967081081081082|   1.280857142857143|\n",
      "|    3|United States|     Concord|     -122.03|      37.78|  11.944357575757572|  0.5757030303030299|\n",
      "|    3|United States|      Dayton|      -83.24|      39.38|   4.976736641221376|  1.4362671755725183|\n",
      "|   12|United States|Fort Collins|     -104.38|      40.99|  -2.595175000000001|   1.179285000000001|\n",
      "|    9|United States|  Fort Wayne|      -85.21|      40.99|   17.67741221374045|  1.1262633587786273|\n",
      "|    4|United States|    Glendale|      -118.7|      34.56|  13.934545454545448|  0.7384000000000003|\n",
      "|    6|United States|    Hartford|      -72.43|      40.99|  18.422961538461536|  1.3193153846153842|\n",
      "|    5|United States|     Killeen|      -98.01|      31.35|   22.60256185567012|    0.76109793814433|\n",
      "|    9|United States|     Lansing|      -85.09|      42.59|  16.395988549618316|  1.1140877862595409|\n",
      "|   11|United States|     Norwalk|      -118.7|      34.56|  12.716219512195117|  0.7552743902439021|\n",
      "|    2|United States|    Paradise|     -115.36|      36.17|   8.457882352941176|  0.7127529411764704|\n",
      "|    5|United States|       Plano|       -96.7|      32.95|  22.243216494845363|  0.6879432989690722|\n",
      "|    6|United States|  Providence|       -72.0|      42.59|  18.105138461538463|  1.3261076923076933|\n",
      "|    4|United States|       Salem|     -122.98|       44.2|   8.466176795580111|  0.7106629834254142|\n",
      "|    1|United States|   Santa Ana|     -117.77|      32.95|  13.115357575757574|  0.8499878787878787|\n",
      "|    6|United States|      Topeka|      -95.72|      39.38|  22.931823529411762|  0.9895158371040722|\n",
      "|    5|United States|        Waco|      -98.01|      31.35|   22.60256185567012|    0.76109793814433|\n",
      "+-----+-------------+------------+------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample the city temperature aggregation\n",
    "groupByGlobalLandTemperaturesByCityDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "![title](data_model_i94_analytics.JPG)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Steps to pipeline the data into the data model\n",
    "\n",
    "1. Drop and re-create the spark tables needed using hive.\n",
    "2. Read the us-cities demographics data\n",
    "2. Clean the us-cities demographics data\n",
    "3. Aggregate the us-cities demographics data by state\n",
    "3. Load the us-cities demographics data to table state_demographics.\n",
    "\n",
    "4. Read the airport codes data\n",
    "5. Cleanup and aggregate the airport data\n",
    "6. Align the coordinates data for joining.\n",
    "\n",
    "7. Read the Global Cities Temperature Data.\n",
    "8. Cleanup and aggreage the temperature data.\n",
    "9. Align the coordinates data for joining.\n",
    "\n",
    "10. Perform a left join of Airport Codes data and the Cities Temperature. This ensures all airport data is retained even if no temperature data for that city is available.\n",
    "11. Load this date to the i94port_cities_mon_avg_temps table.\n",
    "\n",
    "12. Read and cleanup the I94 travel data.\n",
    "13. Drop unneeded columns and group by the year, month, airport and addres.\n",
    "14. Load the fact table df_i94_aggregates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get spark session\n",
    "spark = getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop tables if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS state_demographics\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS df_i94_aggregates\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS i94port_cities_mon_avg_temps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define tables\n",
    "create_state_demographics_sql = \"\"\"CREATE TABLE IF NOT EXISTS state_demographics (\n",
    "                                      state_code STRING, \n",
    "                                      male_population BIGINT, \n",
    "                                      female_population BIGINT, \n",
    "                                      total_population BIGINT, \n",
    "                                      num_of_vets BIGINT, \n",
    "                                      foreign_born BIGINT) USING hive\"\"\"\n",
    "\n",
    "create_df_i94_aggregates_sql = \"\"\"CREATE TABLE IF NOT EXISTS df_i94_aggregates (\n",
    "                                     i94yr DOUBLE, \n",
    "                                     i94mon DOUBLE, \n",
    "                                     i94port STRING, \n",
    "                                     i94addr STRING, \n",
    "                                     count BIGINT) USING hive\"\"\"\n",
    "\n",
    "create_i94port_cities_mon_avg_temps_sql = \"\"\"CREATE TABLE IF NOT EXISTS i94port_cities_mon_avg_temps(\n",
    "                                            name STRING,\n",
    "                                            state STRING,\n",
    "                                            city STRING,\n",
    "                                            i94prtl STRING,\n",
    "                                            month BIGINT,\n",
    "                                            avg_temp FLOAT,\n",
    "                                            avg_temp_uncertainty FLOAT) USING hive\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tables\n",
    "spark.sql(create_state_demographics_sql)\n",
    "spark.sql(create_df_i94_aggregates_sql)\n",
    "spark.sql(create_i94port_cities_mon_avg_temps_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ETL for state demographics data\n",
    "# Get spark session\n",
    "sparkSession = getSparkSession()\n",
    "# Read, transform and load city demographics data\n",
    "cityDemographicsDataDF = readCSVtoSparkDF(sparkSession,\"us-cities-demographics.csv\",';',True,True)\n",
    "cleanedCityDemographicsDataDF = cleanCityDemographicsData(cityDemographicsDataDF)\n",
    "cleanedCityDemographicsDataDF.createOrReplaceTempView(\"state_demographics_tmp\")\n",
    "spark.sql(\"INSERT INTO state_demographics SELECT * FROM state_demographics_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ETL for i94port_cities_mon_avg_temps\n",
    "# Get spark session\n",
    "sparkSession = getSparkSession()\n",
    "# Read airport codes data\n",
    "airportCodesDF = readCSVtoSparkDF(sparkSession,\"airport-codes_csv.csv\",',',True,True)\n",
    "# Read global temperatures by city data\n",
    "globalLandTemperaturesByCityDF = readCSVtoSparkDF(sparkSession,'../../data2/GlobalLandTemperaturesByCity.csv',',',True,True)\n",
    "# Clean airport codes data\n",
    "cleanedAirportCodesDF = cleanAirportCodes(airportCodesDF)\n",
    "# Clean global temperatures by city data\n",
    "filteredGlobalLandTemperaturesByCityDF = filterGlobalTemperaturesByCityDF(globalLandTemperaturesByCityDF)\n",
    "replacedGlobalLandTemperaturesByCityDF = replaceGlobalLandTemperaturesByCity(filteredGlobalLandTemperaturesByCityDF)\n",
    "groupByGlobalLandTemperaturesByCityDF = groupByGlobalLandTemperaturesByCity(replacedGlobalLandTemperaturesByCityDF)\n",
    "# Join airport and temperature data and further cleanup\n",
    "i95port_cities_mon_avg_tempsDF = joinAndCleanupAirportAndTempData(cleanedAirportCodesDF,groupByGlobalLandTemperaturesByCityDF)\n",
    "# load\n",
    "i95port_cities_mon_avg_tempsDF.createOrReplaceTempView(\"i94port_cities_mon_avg_temps_tmpview\")\n",
    "spark.sql(\"INSERT INTO i94port_cities_mon_avg_temps SELECT * FROM i94port_cities_mon_avg_temps_tmpview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ETL for df_i94_aggregates\n",
    "# Get spark session\n",
    "sparkSession = getSparkSession()\n",
    "# List of data files\n",
    "fnames = [\n",
    "    '../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
    "    '../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat'\n",
    "]\n",
    "# Read, Transform and Load\n",
    "for fname in fnames:\n",
    "    I94TravelDataDF = readSAS7BDATtoSparkDF(sparkSession,fname)\n",
    "    cleanedI94TravelDataDF = cleanI94TravelData(I94TravelDataDF)\n",
    "    cleanedI94TravelDataDF.createOrReplaceTempView(\"i94tempTable\")\n",
    "    spark.sql(\"INSERT INTO df_i94_aggregates SELECT * FROM i94tempTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+----------------+-----------+------------+\n",
      "|state_code|male_population|female_population|total_population|num_of_vets|foreign_born|\n",
      "+----------+---------------+-----------------+----------------+-----------+------------+\n",
      "|        IL|        2218541|          2343771|         4562312|     146701|      941735|\n",
      "|        ME|          31480|            35392|           66872|       3666|        9229|\n",
      "|        DE|          32680|            39277|           71957|       3063|        3336|\n",
      "|        OH|        1177546|          1256143|         2433689|     127372|      175219|\n",
      "|        LA|         626998|           673597|         1300595|      69771|       83419|\n",
      "|        NM|         409010|           430032|          839042|      60474|       89112|\n",
      "|        IA|         361176|           372635|          733811|      39898|       63687|\n",
      "|        OK|         714573|           734422|         1448995|      95468|      151174|\n",
      "|        AL|         497248|           552381|         1049629|      71543|       52154|\n",
      "|        NY|        4692055|          5123571|         9815626|     204901|     3438081|\n",
      "|        IN|         910346|           972407|         1882753|      92165|      147233|\n",
      "|        WA|        1245605|          1254502|         2500107|     153126|      440962|\n",
      "|        ND|          95235|            94255|          189490|      10299|       11492|\n",
      "|        WI|         688803|           724342|         1413145|      61134|      124687|\n",
      "|        SD|         122718|           122380|          245098|      16087|       15309|\n",
      "|        DC|         319705|           352523|          672228|      25963|       95117|\n",
      "|        KS|         564145|           584129|         1148274|      64789|      118645|\n",
      "|        AK|         152945|           145750|          298695|      27492|       33258|\n",
      "|        CT|         432157|           453424|          885581|      24953|      225866|\n",
      "|        NE|         357333|           363900|          721233|      39197|       71221|\n",
      "+----------+---------------+-----------------+----------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sample the data\n",
    "spark.sql(\"SELECT * FROM state_demographics\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+-------+-----+----------+--------------------+\n",
      "|                name|state|        city|i94prtl|month|  avg_temp|avg_temp_uncertainty|\n",
      "+--------------------+-----+------------+-------+-----+----------+--------------------+\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|   10| 22.314194|           1.2847413|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    3|   17.5267|           1.4525381|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    8| 26.977701|              1.1355|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|   12|15.2025175|           1.6224138|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|   11|   18.5359|           1.4591423|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    7| 26.932032|           1.2254186|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    2| 15.430149|            1.696229|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    5|  23.44645|           1.1750898|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    6| 25.963415|           1.2348192|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    1| 14.459555|           1.7635665|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    4| 20.224981|           1.3336923|\n",
      "|Jacksonville Exec...|US-FL|Jacksonville|    CRG|    9| 25.831512|           1.1760346|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|   10| 22.314194|           1.2847413|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|    3|   17.5267|           1.4525381|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|    8| 26.977701|              1.1355|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|   12|15.2025175|           1.6224138|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|   11|   18.5359|           1.4591423|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|    7| 26.932032|           1.2254186|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|    2| 15.430149|            1.696229|\n",
      "|Jacksonville Inte...|US-FL|Jacksonville|    JAX|    5|  23.44645|           1.1750898|\n",
      "+--------------------+-----+------------+-------+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sample the data\n",
    "spark.sql(\"SELECT * FROM i94port_cities_mon_avg_temps\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------+-----+\n",
      "| i94yr|i94mon|i94port|i94addr|count|\n",
      "+------+------+-------+-------+-----+\n",
      "|2016.0|   8.0|    SPM|     MN| 8026|\n",
      "|2016.0|   8.0|    SAI|     NH|   35|\n",
      "|2016.0|   8.0|    ELP|     TX|  110|\n",
      "|2016.0|   8.0|    ATL|     NY| 4583|\n",
      "|2016.0|   8.0|    BOS|     OH|  842|\n",
      "|2016.0|   8.0|    SEA|     DE|  166|\n",
      "|2016.0|   8.0|    DAL|     TR|   39|\n",
      "|2016.0|   8.0|    DEN|     TN|   28|\n",
      "|2016.0|   8.0|    XXX|     VA|   36|\n",
      "|2016.0|   8.0|    ATL|     SO|   54|\n",
      "|2016.0|   8.0|    DEN|     AK|   32|\n",
      "|2016.0|   8.0|    ORL|     UN|   53|\n",
      "|2016.0|   8.0|    VCV|   null| 1038|\n",
      "|2016.0|   8.0|    POO|     PA|   18|\n",
      "|2016.0|   8.0|    FTL|     WV|   20|\n",
      "|2016.0|   8.0|    JAC|     GA|    3|\n",
      "|2016.0|   8.0|    MAA|     RI|    1|\n",
      "|2016.0|   8.0|    MAF|     WI|    1|\n",
      "|2016.0|   8.0|    HIG|     PR|    3|\n",
      "|2016.0|   8.0|    CLE|     GA|    1|\n",
      "+------+------+-------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sample the data\n",
    "spark.sql(\"SELECT * FROM df_i94_aggregates\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data quality checks to ensure the pipeline ran as expected. These include:\n",
    "- Unit test the scripts by sampling the tables to verify actual data was loaded.\n",
    "- Doing counts to verify approximate expected row counts\n",
    "- Join tables to actually get a complete record for sample cities\n",
    "- Validate the complete record is unique by month for completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check existence of the table\n",
    "print (checkTableExists(\"state_demographics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check existence of the table\n",
    "print (checkTableExists(\"i94port_cities_mon_avg_temps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check existence of the table\n",
    "print (checkTableExists(\"df_i94_aggregates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check row count matches for a given table\n",
    "print (checkTableRowCountMatches(\"state_demographics\",49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check that we have all 12 months of data\n",
    "print (checkDistinctCountByColumnMatches(\"i94port_cities_mon_avg_temps\",\"month\",12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check that we have all 12 months of data\n",
    "print (checkDistinctCountByColumnMatches(\"df_i94_aggregates\",\"i94mon\",12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Table: df_i94_aggregates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "| i94yr   | The year of travel. From dt field of i94 SAS files\n",
    "| i94mon  | The month of travel. From dt field of i94 SAS files\n",
    "| i94port | The port of departure three letter code. From i94port field of i94 SAS files\n",
    "| i94addr | The address state of immigrant as filled in i94 form.\n",
    "| count   | The aggregated sum of depertures at a give port for a given month and who are from a given address code. Calculated by aggregating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Table: state_demographics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "| state_code         | The state two letter representation from us-cities-demographics data.\n",
    "| male_population    | Aggregated male population per state code.\n",
    "| female_population  | Aggregated female population per state code.\n",
    "| total_population   | Aggregated total population per state code.\n",
    "| num_of_vets        | Aggregated population of veerans in the state.\n",
    "| foreign_born       | Aggregated population of foreign born persons in the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Table: i94port_cities_mon_avg_temps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "| i94prtl              | The iata_code of a port or airport. From the airport-codes data.\n",
    "| name                 | Common name of the airport\n",
    "| city                 | The municipality where the airport is located.\n",
    "| state                | State where airport is located.\n",
    "| month                | month of temperature where there data is available. From the global cities temperatures data.\n",
    "| avg_temp             | average temperature of the city for the given month. From the global cities temperatures data.\n",
    "| avg_temp_uncertainty | average temperature uncertainty of the city for the given month. From the global cities temperatures data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Tools\n",
    "For data exploration, python pandas was sufficient and efficient for small data sets like the airport codes. For larger data set and requiring lots of groupby and analytical queries,  apache spark is the appropriate tool\n",
    "\n",
    "For the global temperature data, Apache Spark is needed because of the volume and the amount of work to get it to align with cities data.\n",
    "\n",
    "Enabling Hive in spark enabled creation of tables are using performing SQL directly on in memory spark tables.\n",
    "\n",
    "\n",
    "#### Frequency of Update\n",
    "The fact table data should be updated at least monthly so as to recieve the new data for the month.\n",
    "The city demographics data should be updated as frequently as census is done or other trusted population estimation data is recieved.\n",
    "The temperature data can be updated every few years.\n",
    "The airport date should be updated every few years as well or whenever a few file is delivered.\n",
    "\n",
    "\n",
    "#### Preparing for 100x data growth\n",
    "Incase of 100x growht in amount of data, we need to implement a spark cluster with 12 workers so each months data is handled by a different worker. The data is delianted well by month and minimal cross referencing so this will improve efficiency and enable processing with reasonable available memory on the wokrkers.\n",
    "If a larger cluster is needed, we need to further partition the data by month and by airport. This will take advantage of even larger Apache Clusters beyond just 12 workers.\n",
    "\n",
    "\n",
    "#### Updating Dashboard by 7am\n",
    "I proopose we implement the ETL via airflow because of the SLA of 7am. Airflow allows re-tries incase of any failures to improve likelihood of completing the tasks by 7am. Additionally, it also allows us to paralleliized tasks we can do in parallel easily. In addition, it has alarming capability to alert us incase of failure and inability to continue.\n",
    "\n",
    "#### Database User base is 100 People\n",
    "For this we need to write out the tables and load it to a DW like Redshift in AWS Cloud. This will allow many analytical queries to simulteneuously be run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
